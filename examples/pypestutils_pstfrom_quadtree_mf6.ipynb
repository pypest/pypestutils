{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a PEST interface from MODFLOW6 using the `PstFrom` class and `pypestutils` using a quadtree MF6 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will combine the power of `pypestutils` with the power of `PstFrom`.  Then we will also demonstrate a technique to condition the prior parameter ensemble to imprecise/uncertain \"direct\" parameter data. Hold on tight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses `flopy`, `pyEMU`, `modflow6` and `pestpp-ies`.  Let's make sure we can import those deps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemu\n",
    "import flopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use methods `get_modflow()` and `get_pestpp()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dir = \"quadtree_bin\"\n",
    "if os.path.exists(bin_dir):\n",
    "   shutil.rmtree(bin_dir)\n",
    "os.makedirs(bin_dir)\n",
    "flopy.utils.get_modflow(bin_dir,downloads_dir=bin_dir)\n",
    "pyemu.utils.get_pestpp(bin_dir,downloads_dir=bin_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An existing MODFLOW6 model is in the directory `freyberg_mf6`.  Lets check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model_ws = os.path.join('freyberg_quadtree')\n",
    "os.listdir(org_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the input array and list data for this model have been written \"externally\" - this is key to using the `PstFrom` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.disv_idomain_layer3.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's copy those files to a temporary location just to make sure we don't goof up those original files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_ws = \"temp_ppu_quadtree\"\n",
    "if os.path.exists(tmp_model_ws):\n",
    "    shutil.rmtree(tmp_model_ws)\n",
    "shutil.copytree(org_model_ws,tmp_model_ws)\n",
    "for bin_file in os.listdir(bin_dir):\n",
    "    shutil.copy2(os.path.join(bin_dir,bin_file),os.path.join(tmp_model_ws,bin_file))\n",
    "os.listdir(tmp_model_ws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need just a tiny bit of info about the spatial discretization of the model - this is needed to work out separation distances between parameters for build a geostatistical prior covariance matrix later.\n",
    "\n",
    "Here we will load the flopy sim and model instance just to help us define some quantities later - flopy is not required to use the `PstFrom` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_model_ws)\n",
    "m = sim.get_model(\"freyberg6\")\n",
    "m.modelgrid.set_coord_info(angrot=-55) #for some reason flopy isnt picking up angrot from the disv options...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dis.top.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_arr = m.dis.top.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate a `PstFrom` class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ws = \"freyberg6_unstruct_template\"\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_model_ws, new_d=template_ws,\n",
    "                 remove_existing=True,\n",
    "                 longnames=True,\n",
    "                 zero_based=False,start_datetime=\"1-1-2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "So now that we have a `PstFrom` instance, but its just an empty container at this point, so we need to add some PEST interface \"observations\" and \"parameters\".  Let's start with observations using MODFLOW6 head.  These are stored in `heads.csv`.  Note the zero-based layer-row-column name is stored in the site names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsdf = pd.read_csv(os.path.join(tmp_model_ws,\"heads.csv\"),index_col=0)\n",
    "hdsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main entry point for adding observations is (surprise) `PstFrom.add_observations()`.  This method works on the list-type observation output file.  We need to tell it what column is the index column (can be string if there is a header or int if no header) and then what columns contain quantities we want to monitor (e.g. \"observe\") in the control file - in this case we want to monitor all columns except the index column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df = pf.add_observations(\"heads.csv\",insfile=\"heads.csv.ins\",index_cols=\"time\",\n",
    "                    use_cols=list(hdsdf.columns.values),prefix=\"hds\",)\n",
    "hds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it returned a dataframe with lots of useful info: the observation names that were formed (`obsnme`), the values that were read from `heads.csv` (`obsval`) and also some generic weights and group names.  At this point, no control file has been created, we have simply prepared to add this observations to the control file later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  We also have a PEST-style instruction file for those obs.\n",
    "\n",
    "Now lets do the same for SFR observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(tmp_model_ws, \"sfr.csv\"), index_col=0)\n",
    "sfr_df = pf.add_observations(\"sfr.csv\", insfile=\"sfr.csv.ins\", index_cols=\"time\", use_cols=list(df.columns.values))\n",
    "sfr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet as!  Now that we have some observations, let's add parameters!\n",
    "\n",
    "## Parameters\n",
    "\n",
    "In the `PstFrom` realm, all parameters are setup as multipliers against existing array and list files.  This is a good thing because it lets us preserve the existing model inputs and treat them as the mean of the prior parameter distribution. It also let's us use mixtures of spatial and temporal scales in the parameters to account for varying scale of uncertainty. \n",
    "\n",
    "Since we are all sophisticated and recognize the importance of expressing spatial and temporal uncertainty (e.g. heterogeneity) in the model inputs (and the corresponding spatial correlation in those uncertain inputs), let's use geostatistics to express uncertainty.  But lets use the awesome sauce in `PyPestUtils` to do fancy things with our pilot point parameters.  To start, we need pilot point location information.  We can generate this ourselves however we want - we need spatial info (ie \"x\" and \"y\" coordinates) and we need at least default values for \"value\" and \"zone\".  If we want to have spatially-varying geostatistical hyper parameters, then we also need columns for \"bearing\",\"aniso\", and \"corrlen\", which are the bearing (duh), anisotropy ratio and correlation length at each pilot point location.  Like we said, you can generate this however works best for you - maybe in a GIS or with geopandas, whatevs.  For this notebook, we will use the same (or similar) pilot point info that was used in the structured freyberg `pypestutils` notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypestutils import helpers as ppu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf = pd.read_csv(os.path.join(\"freyberg_aux_files\",\"pp_info.csv\"),index_col=0)\n",
    "ppdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_aspect(\"equal\")\n",
    "#ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,top_arr)\n",
    "#ax = m.dis.top.plot()\n",
    "\n",
    "ax.scatter(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,alpha=0.5,color=\"0.5\")\n",
    "ax.scatter(ppdf.x,ppdf.y,marker=\".\",c=\"r\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf.loc[:,\"value\"] = 3.0  # HK in layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the various geostatistical hyper parameter pilot point attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mask = id_arr.copy()\n",
    "id_mask[id_mask!=0] = np.nan\n",
    "fig,axes = plt.subplots(1,3)\n",
    "for ax,attr in zip(axes,[\"aniso\",\"corrlen\",\"bearing\"]):\n",
    "    #ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,id_mask)\n",
    "    m.dis.top.plot(axes=[ax])\n",
    "    ax.scatter(ppdf.x,ppdf.y,c=ppdf.loc[:,attr])\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(attr,loc=\"left\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet ez!  But how do we interpolate with this fancy-ness?  Well there is a helper for that.  To demo its use, lets gen up some rando \"value\" values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf.loc[:,\"value\"] = np.random.normal(1.0,0.25,ppdf.shape[0])\n",
    "results = ppu.interpolate_with_sva_pilotpoints_2d(ppdf,os.path.join(pf.new_d,\"freyberg6.disv.grb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag,arr in results.items():\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(tag,loc=\"left\")\n",
    "    m.dis.top = arr\n",
    "    ax = m.dis.top.plot(colorbar=True)\n",
    "    \n",
    "    #plt.colorbar(cb)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, thats pretty awesome.  So now lets proceed as usual with `PstFrom`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the idomain array to use as a zone array - this keeps us from setting up parameters in inactive model cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = m.dis.idomain[0].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_arr_files = [f for f in os.listdir(tmp_model_ws) if \"npf_k_\" in f and f.endswith(\".txt\")]\n",
    "hk_arr_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those are the existing model input arrays for HK.  Notice we found the files in the temporary model workspace - `PstFrom` will copy all those files to the new model workspace for us in a bit...\n",
    "\n",
    "Let's setup grid-scale multiplier parameters and fancy pilot point multipliers for HK in all layers.  Typically, we would want pass a `geostruct` to `PstFrom.add_parameters()` for the grid-scale parameters to that later when we generate the prior parameter ensemble, grid-scale parameter realizations would get some spatial correlation. But in this notebook, we are focused on using `pypestutils`, so when we get to the prior parameter ensemble generation, we will use `pypestutils` to generate spatially correlated grid-scale parameter realizations.\n",
    "\n",
    "To start with, lets make sure to reset the \"value\" column in `ppdf` to 1.0 so we can use it as a multiplier...and...this where it gets deep: we need to have some spatial correlation on the pilot point spatial correlation attributes - yeesh!  This is where the term \"hyper-parameter\" comes from:  We are treating the controling attributes of the pilot point interpolation as \"parameters\" in the PEST sense, which means we want those attributes to have some spatial structure to them, otherwise, the resulting property fields will have implausible bearing, aniso, and/or correlation length variability (that is, those quantities will vary sharply over short distances, which we dont want). So we need to define, you guessed it, a geostatistical structure for...the...geostatistical structure?! #inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf.loc[:,\"value\"] = 1.0\n",
    "pp_v = pyemu.geostats.ExpVario(contribution=1.0, a=2000)\n",
    "pp_gs = pyemu.geostats.GeoStruct(variograms=pp_v,transform=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, even though we are telling `PstFrom` that the pilot point parameters are \"direct\" type, we are actually going to use them as multipliers during the model run process (stay tuned for that part).  To do this, we need to track the model filenames and pilot point csv filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_files,mod_files = [],[]\n",
    "for hk_arr_file in hk_arr_files:\n",
    "    layer = int(hk_arr_file.split(\".\")[1].split(\"layer\")[1])\n",
    "    base = hk_arr_file.split('.')[1].replace(\"_\",\"\")+\"_attr:\"\n",
    "    pf.add_parameters(filenames=hk_arr_file,par_type=\"grid\",\n",
    "                       par_name_base=base+\"gr\",pargp=base+\"gr\",zone_array=np.atleast_2d(ib).transpose(),\n",
    "                       upper_bound=2.,lower_bound=0.5,ult_ubound=100,ult_lbound=0.01)\n",
    "    pppdf = ppdf.copy()\n",
    "    \n",
    "    pppdf.loc[:,\"name\"] = [n for n in pppdf.ppname.values]\n",
    "    pppdf.loc[:,\"ppname\"] = pppdf.name.values\n",
    "    pp_file = os.path.join(pf.new_d,base+\"pp.csv\")\n",
    "    pppdf.to_csv(pp_file,index=False)\n",
    "    pp_files.append(os.path.split(pp_file)[1])\n",
    "    mod_files.append(hk_arr_file)\n",
    "    df = pf.add_parameters(os.path.split(pp_file)[1],par_type=\"grid\",index_cols={\"ppname\":\"ppname\",\"x\":\"x\",\"y\":\"y\"},\n",
    "        use_cols=[\"value\",\"bearing\",\"aniso\",\"corrlen\"],\n",
    "        par_name_base=[base+\"pp\",base+\"bearing\",base+\"aniso\",base+\"corrlen\"],\n",
    "        pargp=[base+\"pp\",base+\"bearing\",base+\"aniso\",base+\"corrlen\"],\n",
    "        upper_bound=[20,pppdf.bearing.max()*1.1,pppdf.aniso.max()*1.1,pppdf.corrlen.max()*1.1],\n",
    "        lower_bound=[.05,pppdf.bearing.min()*.9,pppdf.aniso.min()*.9,pppdf.corrlen.min()*.9],\n",
    "        par_style=\"direct\",transform=\"log\",geostruct=pp_gs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files = [f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]\n",
    "tpl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws,tpl_files[0]),'r') as f:\n",
    "    for _ in range(4):\n",
    "        print(f.readline().strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the `pypestutils` pilot point interpolation into the foward run process\n",
    "\n",
    "So we added extra fancy pilot points to our pest interface with the smoothness of `PstFrom`.  But we still need to interpolate the pilot point values using the associated (estimated) hyper-parameters to the model grid at runtime.  To do this, we have setup a little helper funtion in a python script called \"ppu_helpers.py\". This helper function will read the \".grb\" file at runtime so we need to make sure it always available. So let's copy the current \".grb\" file to a file for safe keeping (in case MODFLOW-6 fails to run to competion and we lose the \".grb\" file for the next run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(os.path.join(pf.new_d,\"freyberg6.disv.grb\"),os.path.join(pf.new_d,\"org.grb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(\"ppu_helpers.py\",'r').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just need to call this little function after the `PstFrom` runtime mult-to-model process happens so that we interpolate the pilot points to a grid-shaped array, then pickup the HK arrays that were created and multiply them by the interpolated array, and save for MODFLOW to see.  This is easy with `PstFrom`.  But first let's see if the function actually works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pp file and model file info\n",
    "df = pd.DataFrame({\"model_file\":mod_files,\"pp_file\":pp_files})\n",
    "df.to_csv(os.path.join(pf.new_d,\"pp_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppu_helpers\n",
    "ppu_helpers.setup_pps(pf.new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_py_function(\"ppu_helpers.py\",\"apply_pps()\",is_pre_cmd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For added fun, lets track interpolated pp array and the log of the model input array as observations, just so we can see what its doing...(and we can also use these \"observations\" later for conditioning the parameter realizations to HK measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_files = [f for f in os.listdir(pf.new_d) if f.endswith(\".txt\") and f.startswith(\"interp\")]\n",
    "assert len(interp_files) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interp_file in interp_files:\n",
    "    base = interp_file.replace(\"_\",\"\").replace(\".\",\"\").replace(\"txt\",\"\")\n",
    "    pf.add_observations(interp_file,prefix=base,obsgp=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_files = [f for f in os.listdir(pf.new_d) if f.endswith(\".txt\") and f.startswith(\"log_\")]\n",
    "assert len(log_files) == 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_file in log_files:\n",
    "    base = log_file.replace(\"_\",\"\").replace(\".\",\"\").replace(\"txt\",\"\")\n",
    "    pf.add_observations(log_file,prefix=base,obsgp=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the control file, pest interface files, and forward run script\n",
    "At this point, we have some parameters and some observations, so we can create a control file...but first lets make sure we have set `mf6` as the model run command in `PstFrom`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this conditional is just in case this block gets run multiple times...\n",
    "if \"mf6\" not in pf.mod_sys_cmds:\n",
    "    pf.mod_sys_cmds.append(\"mf6\")\n",
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that magical forward run script that `PstFrom` writes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After building the control file\n",
    "\n",
    "At this point, we can do some additional modifications that would typically be done that are problem specific.  Note that any modifications made after calling `PstFrom.build_pst()` will only exist in memory - you need to call `pf.pst.write()` to record these changes to the control file on disk.  Also note that if you call `PstFrom.build_pst()` after making some changes, these changes will be lost.  \n",
    "\n",
    "Once you think you are happy with the initial interface design, the famous `noptmax=0` test is in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(pf.new_d,\"freyberg.pst\"),version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a prior parameter ensemble\n",
    "\n",
    "As usual, we will use `PstFrom`'s awesome `draw()` method to generate our prior parameter ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reals = 50\n",
    "pe = pf.draw(num_reals=num_reals,use_specsim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid-scale parameter realizations with `pypestutils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "So that's kewl... but its important to notice that only the pilot-point-related parameter groups were drawn using a full covariance matrix.  This is because earlier, we did not pass a `geostruct` to `PstFrom.add_parameters()`.  Let's see how to generate grids-scale parameter realizations with `pypestutils` (note this approach is more general and support unstructed grids also...).  First using default arguments for the geostatistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig,ax = plt.subplots(1,1)\n",
    "#ax.set_aspect(\"equal\")\n",
    "#ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,results[0])\n",
    "m.dis.top = results[0]\n",
    "m.dis.top.plot()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use an off-grid angle bearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals,variobearing=90,varioaniso=3)\n",
    "m.dis.top = results[0]\n",
    "m.dis.top.plot(colorbar=True)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping `pypestutils` realizations into the prior parameter ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.pargp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_groups = [\"npfklayer1_attr:gr\",\"npfklayer2_attr:gr\",\"npfklayer3_attr:gr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for igrp,grp in enumerate(grid_groups):\n",
    "    grpar = par.loc[par.pargp==grp,:].copy()\n",
    "    assert grpar.shape[0] > 0\n",
    "    grpar[\"i\"] = grpar.i.astype(int)\n",
    "    \n",
    "    names,ivals,jvals = grpar.parnme.values,grpar.i.values,grpar.j.values\n",
    "    results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals,\n",
    "                                            variobearing=1.0,varioaniso=1.0,variorange=1000,variance=0.0125,mean=0.0,random_seed=12345*igrp)\n",
    "    print(grpar.shape)\n",
    "    for ireal,real in enumerate(results):\n",
    "        real = real[id_arr>0]\n",
    "        #print(real.shape,ivals.shape,jvals.shape)\n",
    "        pe.loc[pe.index[ireal],names] = 10**(real)\n",
    "    print(\"group bound info: \",grpar.parlbnd.unique(),grpar.parubnd.unique())\n",
    "    print(\"ensemble range info: \",pe.loc[:,names].values.min(),pe.loc[:,names].values.max())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros_like(id_arr,dtype=float)\n",
    "arr[id_arr>0] = pe.loc[pe.index[0],names].values\n",
    "print(arr)\n",
    "m.dis.top = arr\n",
    "m.dis.top.plot(colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the new realizations seems to jive with the parameter bounds (since we chose it that way...).  \n",
    "\n",
    "So let's re-save the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.enforce()\n",
    "pe.to_csv(os.path.join(template_ws,\"prior.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we made it!  This should have you in a good place to move forward with setting observation values and weights and running some stochastic unstructured mf6 runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
