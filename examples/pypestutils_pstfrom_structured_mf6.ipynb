{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a PEST interface from MODFLOW6 using the `PstFrom` class and `pypestutils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will combine the power of `pypestutils` with the power of `PstFrom`.  Then we will also demonstrate a technique to condition the prior parameter ensemble to imprecise/uncertain \"direct\" parameter data. Hold on tight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses `flopy`, `pyEMU`, `modflow6` and `pestpp-ies`.  Let's make sure we can import those deps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemu\n",
    "import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dir = \"struct_bin\"\n",
    "if os.path.exists(bin_dir):\n",
    "   shutil.rmtree(bin_dir)\n",
    "os.makedirs(bin_dir)\n",
    "flopy.utils.get_modflow(bin_dir,downloads_dir=bin_dir)\n",
    "pyemu.utils.get_pestpp(bin_dir,downloads_dir=bin_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An existing MODFLOW6 model is in the directory `freyberg_mf6`.  Lets check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model_ws = os.path.join('freyberg_monthly')\n",
    "os.listdir(org_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the input array and list data for this model have been written \"externally\" - this is key to using the `PstFrom` class. \n",
    "\n",
    "Let's quickly viz the model top just to remind us of what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.dis_idomain_layer3.txt\"))\n",
    "top_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.dis_top.txt\"))\n",
    "top_arr[id_arr==0] = np.nan\n",
    "plt.imshow(top_arr)\n",
    "top_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's copy those files to a temporary location just to make sure we don't goof up those original files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_ws = \"temp_ppu_struct\"\n",
    "if os.path.exists(tmp_model_ws):\n",
    "    shutil.rmtree(tmp_model_ws)\n",
    "shutil.copytree(org_model_ws,tmp_model_ws)\n",
    "for bin_file in os.listdir(bin_dir):\n",
    "    shutil.copy2(os.path.join(bin_dir,bin_file),os.path.join(tmp_model_ws,bin_file))\n",
    "os.listdir(tmp_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need just a tiny bit of info about the spatial discretization of the model - this is needed to work out separation distances between parameters for build a geostatistical prior covariance matrix later.\n",
    "\n",
    "Here we will load the flopy sim and model instance just to help us define some quantities later - flopy is not required to use the `PstFrom` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_model_ws)\n",
    "m = sim.get_model(\"freyberg6\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate a `PstFrom` class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ws = \"freyberg6_template\"\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_model_ws, new_d=template_ws,\n",
    "                 remove_existing=True,\n",
    "                 longnames=True, spatial_reference=m.modelgrid,\n",
    "                 zero_based=False,start_datetime=\"1-1-2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "So now that we have a `PstFrom` instance, but its just an empty container at this point, so we need to add some PEST interface \"observations\" and \"parameters\".  Let's start with observations using MODFLOW6 head.  These are stored in `heads.csv`.  Note the zero-based layer-row-column name is stored in the site names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsdf = pd.read_csv(os.path.join(tmp_model_ws,\"heads.csv\"),index_col=0)\n",
    "hdsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main entry point for adding observations is (surprise) `PstFrom.add_observations()`.  This method works on the list-type observation output file.  We need to tell it what column is the index column (can be string if there is a header or int if no header) and then what columns contain quantities we want to monitor (e.g. \"observe\") in the control file - in this case we want to monitor all columns except the index column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df = pf.add_observations(\"heads.csv\",insfile=\"heads.csv.ins\",index_cols=\"time\",\n",
    "                    use_cols=list(hdsdf.columns.values),prefix=\"hds\",)\n",
    "hds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it returned a dataframe with lots of useful info: the observation names that were formed (`obsnme`), the values that were read from `heads.csv` (`obsval`) and also some generic weights and group names.  At this point, no control file has been created, we have simply prepared to add this observations to the control file later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  We also have a PEST-style instruction file for those obs.\n",
    "\n",
    "Now lets do the same for SFR observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(tmp_model_ws, \"sfr.csv\"), index_col=0)\n",
    "sfr_df = pf.add_observations(\"sfr.csv\", insfile=\"sfr.csv.ins\", index_cols=\"time\", use_cols=list(df.columns.values))\n",
    "sfr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet as!  Now that we have some observations, let's add parameters!\n",
    "\n",
    "## Parameters\n",
    "\n",
    "In the `PstFrom` realm, all parameters are setup as multipliers against existing array and list files.  This is a good thing because it lets us preserve the existing model inputs and treat them as the mean of the prior parameter distribution. It also let's us use mixtures of spatial and temporal scales in the parameters to account for varying scale of uncertainty. \n",
    "\n",
    "Since we are all sophisticated and recognize the importance of expressing spatial and temporal uncertainty (e.g. heterogeneity) in the model inputs (and the corresponding spatial correlation in those uncertain inputs), let's use geostatistics to express uncertainty.  But lets use the awesome sauce in `PyPestUtils` to do fancy things with our pilot point parameters.  To start, we need pilot point location information.  We can generate this ourselves however we want - we need spatial info (ie \"x\" and \"y\" coordinates) and we need at least default values for \"value\" and \"zone\".  If we want to have spatially-varying geostatistical hyper parameters, then we also need columns for \"bearing\",\"aniso\", and \"corrlen\", which are the bearing (duh), anisotropy ratio and correlation length at each pilot point location.  Like we said, you can generate this however works best for you - maybe in a GIS or with geopandas, whatevs.  `PyPestUtils` has a very (very) simple helper that can generate 2D pilot point locations for structured grids by use of a `pp_space` argument, which is simply the number of rows and columns between pilot points.  Trivial..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypestutils import helpers as ppu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first call, we just pass the pilot point generator the most basic info: just the modflow6 binary grid file (the \".grb\" file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf = ppu.get_2d_pp_info_structured_grid(10,os.path.join(pf.new_d,\"freyberg6.dis.grb\"))\n",
    "ppdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_aspect(\"equal\")\n",
    "#ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,top_arr)\n",
    "ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,top_arr)\n",
    "ax.scatter(ppdf.x,ppdf.y,marker=\".\",c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well thats ok, but we dont really want pilot points in the inactive zones, do we?  Let's try this again, this time pass the `idomain` as the \"zone\" array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf = ppu.get_2d_pp_info_structured_grid(10,os.path.join(pf.new_d,\"freyberg6.dis.grb\"),array_dict={\"zone\":m.dis.idomain.array[0,:,:]})\n",
    "ppdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,top_arr)\n",
    "ax.scatter(ppdf.x,ppdf.y,marker=\".\",c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho gusto!  Thats better.  We can also pass other array quantities to this simple helper if we want to sample arrays to the pilot point values (including the quantities we modify for the pilot point below).  Let's do something fun (and quasi-geologic?): Let's assume that in the vicinity of the SFR boundary, there are buried meander-shaped deposits, but that as we move away from the SFR boundary, more traditional two-point geostatistical heterogeneity exists.  Sounds fancy right?  We can do this with spatially varying geostatistical hyper-parameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets set the \"value\", which is simply the property value.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf.loc[:,\"value\"] = 3.0  # HK in layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aniso to be a function of column value\n",
    "# stronger aniso near the sfr network\n",
    "jmin,jmax = ppdf.j.min(),float(ppdf.j.max())\n",
    "ppdf.loc[:,\"aniso\"] = 30 * ppdf.j.values.copy() / jmax\n",
    "ppdf.loc[ppdf.aniso<1,\"aniso\"] = 1\n",
    "ppdf.loc[ppdf.aniso>10,\"aniso\"] = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for corr len - longer correlations near sfr\n",
    "# cl = ppdf.corrlen.min()\n",
    "# ppdf.loc[:,\"corrlen\"] = cl * (ppdf.j.values.copy() / jmax) * sr.delc.max()\n",
    "# ppdf.loc[ppdf.corrlen<cl/20,\"corrlen\"] = cl/20\n",
    "ppdf.loc[:,\"x\"] = np.round(ppdf.x.values,1)\n",
    "ppdf.loc[:,\"y\"] = np.round(ppdf.y.values,1)\n",
    "ppdf.corrlen *= 10\n",
    "ppdf.corrlen.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set bearing to be a high-freq sin wave in the sfr direction\n",
    "y = np.cumsum(m.dis.delc.array)\n",
    "phase = np.pi/4\n",
    "gain = 40\n",
    "iy = np.linspace(phase,5*np.pi+phase,y.shape[0])\n",
    "ix = 235 + np.sin(iy) * gain\n",
    "ppdf.loc[:,\"bearing\"] = ix[ppdf.i.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize those pilot point attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mask = id_arr.copy()\n",
    "id_mask[id_mask!=0] = np.nan\n",
    "fig,axes = plt.subplots(1,3)\n",
    "for ax,attr in zip(axes,[\"aniso\",\"corrlen\",\"bearing\"]):\n",
    "    #ax.pcolormesh(sr.xcentergrid,sr.ycentergrid,id_mask)\n",
    "    ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,id_mask)\n",
    "    ax.scatter(ppdf.x,ppdf.y,c=ppdf.loc[:,attr])\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(attr,loc=\"left\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet ez!  But how do we interpolate with this fancy-ness?  Well there is a helper for that.  To demo its use, lets gen up some rando \"value\" values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf.to_csv(os.path.join(pf.new_d,\"pp_info.csv\"))\n",
    "ppdf.loc[:,\"value\"] = np.random.normal(1.0,0.25,ppdf.shape[0])\n",
    "results = ppu.interpolate_with_sva_pilotpoints_2d(ppdf,os.path.join(pf.new_d,\"freyberg6.dis.grb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,id_mask)\n",
    "ax.scatter(ppdf.x,ppdf.y,c=ppdf.loc[:,\"value\"])\n",
    "ax.set_aspect(\"equal\")\n",
    "_= ax.set_title(\"pilot point values\")\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "for tag,arr in results.items():\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(tag,loc=\"left\")\n",
    "    cb = ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,arr)\n",
    "    plt.colorbar(cb)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, thats pretty awesome.  So now lets proceed as usual with `PstFrom`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the idomain array to use as a zone array - this keeps us from setting up parameters in inactive model cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = m.dis.idomain[0].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_arr_files = [f for f in os.listdir(tmp_model_ws) if \"npf_k_\" in f and f.endswith(\".txt\")]\n",
    "hk_arr_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those are the existing model input arrays for HK.  Notice we found the files in the temporary model workspace - `PstFrom` will copy all those files to the new model workspace for us in a bit...\n",
    "\n",
    "Let's setup grid-scale multiplier parameters and fancy pilot point multipliers for HK in all layers.  Typically, we would want pass a `geostruct` to `PstFrom.add_parameters()` for the grid-scale parameters to that later when we generate the prior parameter ensemble, grid-scale parameter realizations would get some spatial correlation. But in this notebook, we are focused on using `pypestutils`, so when we get to the prior parameter ensemble generation, we will use `pypestutils` to generate spatially correlated grid-scale parameter realizations.\n",
    "\n",
    "To start with, lets make sure to reset the \"value\" column in `ppdf` to 1.0 so we can use it as a multiplier...and...this where it gets deep: we need to have some spatial correlation on the pilot point spatial correlation attributes - yeesh!  This is where the term \"hyper-parameter\" comes from:  We are treating the controling attributes of the pilot point interpolation as \"parameters\" in the PEST sense, which means we want those attributes to have some spatial structure to them, otherwise, the resulting property fields will have implausible bearing, aniso, and/or correlation length variability (that is, those quantities will vary sharply over short distances, which we dont want). So we need to define, you guessed it, a geostatistical structure for...the...geostatistical structure?! #inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppdf.loc[:,\"value\"] = 1.0\n",
    "pp_v = pyemu.geostats.ExpVario(contribution=1.0, a=2000)\n",
    "pp_gs = pyemu.geostats.GeoStruct(variograms=pp_v,transform=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, even though we are telling `PstFrom` that the pilot point parameters are \"direct\" type, we are actually going to use them as multipliers during the model run process (stay tuned for that part).  To do this, we need to track the model filenames and pilot point csv filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_files,mod_files = [],[]\n",
    "for hk_arr_file in hk_arr_files:\n",
    "    layer = int(hk_arr_file.split(\".\")[1].split(\"layer\")[1])\n",
    "    base = hk_arr_file.split('.')[1].replace(\"_\",\"\")+\"_attr:\"\n",
    "    pf.add_parameters(filenames=hk_arr_file,par_type=\"grid\",\n",
    "                       par_name_base=base+\"gr\",pargp=base+\"gr\",zone_array=ib,\n",
    "                       upper_bound=2.,lower_bound=0.5,ult_ubound=100,ult_lbound=0.01)\n",
    "    pppdf = ppdf.copy()\n",
    "    \n",
    "    pppdf.loc[:,\"name\"] = [n for n in pppdf.ppname.values]\n",
    "    pppdf.loc[:,\"ppname\"] = pppdf.name.values\n",
    "    pp_file = os.path.join(pf.new_d,base+\"pp.csv\")\n",
    "    pppdf.to_csv(pp_file,index=False)\n",
    "    pp_files.append(os.path.split(pp_file)[1])\n",
    "    mod_files.append(hk_arr_file)\n",
    "    df = pf.add_parameters(os.path.split(pp_file)[1],par_type=\"grid\",index_cols={\"ppname\":\"ppname\",\"x\":\"x\",\"y\":\"y\"},\n",
    "        use_cols=[\"value\",\"bearing\",\"aniso\",\"corrlen\"],\n",
    "        par_name_base=[base+\"pp\",base+\"bearing\",base+\"aniso\",base+\"corrlen\"],\n",
    "        pargp=[base+\"pp\",base+\"bearing\",base+\"aniso\",base+\"corrlen\"],\n",
    "        upper_bound=[20,pppdf.bearing.max()*1.1,pppdf.aniso.max()*1.1,pppdf.corrlen.max()*1.1],\n",
    "        lower_bound=[.05,pppdf.bearing.min()*.9,pppdf.aniso.min()*.9,pppdf.corrlen.min()*.9],\n",
    "        par_style=\"direct\",transform=\"log\",geostruct=pp_gs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files = [f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]\n",
    "tpl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws,tpl_files[0]),'r') as f:\n",
    "    for _ in range(4):\n",
    "        print(f.readline().strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the `pypestutils` pilot point interpolation into the foward run process\n",
    "\n",
    "So we added extra fancy pilot points to our pest interface with the smoothness of `PstFrom`.  But we still need to interpolate the pilot point values using the associated (estimated) hyper-parameters to the model grid at runtime.  To do this, we have setup a little helper funtion in a python script called \"ppu_helpers.py\".  This helper function will read the \".grb\" file at runtime so we need to make sure it always available.  So let's copy the current \".grb\" file to a file for safe keeping (in case MODFLOW-6 fails to run to competion and we lose the \".grb\" file for the next run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(os.path.join(pf.new_d,\"freyberg6.dis.grb\"),os.path.join(pf.new_d,\"org.grb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(\"ppu_helpers.py\",'r').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just need to call this little function after the `PstFrom` runtime mult-to-model process happens so that we interpolate the pilot points to a grid-shaped array, then pickup the HK arrays that were created and multiply them by the interpolated array, and save for MODFLOW to see.  This is easy with `PstFrom`.  But first let's see if the function actually works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pp file and model file info\n",
    "df = pd.DataFrame({\"model_file\":mod_files,\"pp_file\":pp_files})\n",
    "df.to_csv(os.path.join(pf.new_d,\"pp_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppu_helpers\n",
    "ppu_helpers.setup_pps(pf.new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_py_function(\"ppu_helpers.py\",\"apply_pps()\",is_pre_cmd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For added fun, lets track interpolated pp array and the log of the model input array as observations, just so we can see what its doing...(and we can also use these \"observations\" later for conditioning the parameter realizations to HK measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_files = [f for f in os.listdir(pf.new_d) if f.endswith(\".txt\") and f.startswith(\"interp\")]\n",
    "assert len(interp_files) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interp_file in interp_files:\n",
    "    base = interp_file.replace(\"_\",\"\").replace(\".\",\"\").replace(\"txt\",\"\")\n",
    "    pf.add_observations(interp_file,prefix=base,obsgp=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_files = [f for f in os.listdir(pf.new_d) if f.endswith(\".txt\") and f.startswith(\"log_\")]\n",
    "assert len(log_files) == 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_file in log_files:\n",
    "    base = log_file.replace(\"_\",\"\").replace(\".\",\"\").replace(\"txt\",\"\")\n",
    "    pf.add_observations(log_file,prefix=base,obsgp=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the control file, pest interface files, and forward run script\n",
    "At this point, we have some parameters and some observations, so we can create a control file...but first lets make sure we have set `mf6` as the model run command in `PstFrom`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this conditional is just in case this block gets run multiple times...\n",
    "if \"mf6\" not in pf.mod_sys_cmds:\n",
    "    pf.mod_sys_cmds.append(\"mf6\")\n",
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that magical forward run script that `PstFrom` writes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After building the control file\n",
    "\n",
    "At this point, we can do some additional modifications that would typically be done that are problem specific.  Note that any modifications made after calling `PstFrom.build_pst()` will only exist in memory - you need to call `pf.pst.write()` to record these changes to the control file on disk.  Also note that if you call `PstFrom.build_pst()` after making some changes, these changes will be lost.  \n",
    "\n",
    "Once you think you are happy with the initial interface design, the famous `noptmax=0` test is in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(pf.new_d,\"freyberg.pst\"),version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a prior parameter ensemble\n",
    "\n",
    "As usual, we will use `PstFrom`'s awesome `draw()` method to generate our prior parameter ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reals = 30\n",
    "pe = pf.draw(num_reals=num_reals,use_specsim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid-scale parameter realizations with `pypestutils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "So that's kewl... but its important to notice that only the pilot-point-related parameter groups were drawn using a full covariance matrix.  This is because earlier, we did not pass a `geostruct` to `PstFrom.add_parameters()`.  Let's see how to generate grids-scale parameter realizations with `pypestutils` (note this approach is more general and support unstructed grids also...).  First using default arguments for the geostatistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,results[0])\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use an off-grid angle bearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals,variobearing=90,varioaniso=3)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,results[0])\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember all that fanciness with hyper-parameters?  We can use that here too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some redic bearing that makes a pretty picture...\n",
    "nrow,ncol = m.dis.nrow.data,m.dis.ncol.data\n",
    "bearing = np.add(np.ones((nrow,ncol)),np.atleast_2d(np.arange(ncol)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals,\n",
    "                                            variobearing=bearing,varioaniso=7,variorange=2000,variance=0.025,mean=0.0)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,results[0])\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping `pypestutils` realizations into the prior parameter ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.pargp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_groups = [\"npfklayer1_attr:gr\",\"npfklayer2_attr:gr\",\"npfklayer3_attr:gr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for igrp,grp in enumerate(grid_groups):\n",
    "    grpar = par.loc[par.pargp==grp,:].copy()\n",
    "    assert grpar.shape[0] > 0\n",
    "    grpar[\"i\"] = grpar.i.astype(int)\n",
    "    grpar[\"j\"] = grpar.j.astype(int)\n",
    "    names,ivals,jvals = grpar.parnme.values,grpar.i.values,grpar.j.values\n",
    "    results = ppu.generate_2d_grid_realizations(os.path.join(pf.new_d,\"org.grb\"),num_reals=num_reals,\n",
    "                                            variobearing=1.0,varioaniso=1.0,variorange=1000,variance=0.0125,mean=0.0,random_seed=12345*igrp)\n",
    "    for ireal,real in enumerate(results):\n",
    "        pe.loc[pe.index[ireal],names] = 10**(real[ivals,jvals])\n",
    "    print(\"group bound info: \",grpar.parlbnd.unique(),grpar.parubnd.unique())\n",
    "    print(\"ensemble range info: \",pe.loc[:,names].values.min(),pe.loc[:,names].values.max())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((nrow,ncol))\n",
    "arr[ivals,jvals] = pe.loc[pe.index[0],names]\n",
    "arr[id_arr==0] = np.nan\n",
    "cb = plt.imshow(arr)\n",
    "plt.colorbar(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the new realizations seems to jive with the parameter bounds (since we chose it that way...).  \n",
    "\n",
    "So let's re-save the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.enforce()\n",
    "pe.to_csv(os.path.join(template_ws,\"prior.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus material:  Conditioning the prior parameter ensemble to \"direct\" parameter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We well have setup an PEST interface for the freyberg model, and, this point, traditionally we would jump into some form of history matching...but...what about data related directly to the parameters like aquifer test results?  These data are potentially valuable but including these imprecise/uncertain data into the decision-support modeling process can be dicey.  Some folks like to ignore their existence, while others want to force the model inputs to exactly match these data.  We feel the best approach is somewhere in the middle: data assimilation that recognizes the uncertainties of these data.  We will now demonstrate a workflow to do this assimilation on a hypothetical case using the PEST interface we have construted.  This workflow is highly efficient because to assimilate these direct parameter data we dont actually need to run MODFLOW, so we will modify our PEST interface to remove MODFLOW and its components.  Remember how we added the resultant HK array as \"obseravtions\" earlier?  Well we can treat some of those (ones that are coincident with monitoring well locations) as places were HK has been \"measured\" (in quotes because these measurements are often very imprecise and when scaling for a small diameter monitoring well to a model cell, even more noise is introduced - more on this later).  So first, lets remove the MODFLOW components from the PEST interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(os.path.join(pf.new_d,\"forward_run.py\"),\"r\").readlines()\n",
    "with open(os.path.join(pf.new_d,\"forward_run.py\"),\"w\") as f:\n",
    "    for line in lines:\n",
    "        if \"mf6\" in line.lower():\n",
    "            continue\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(pf.new_d,\"freyberg.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.model_output_data.pest_file.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.drop_observations(os.path.join(pf.new_d,\"heads.csv.ins\"),pst_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.drop_observations(os.path.join(pf.new_d,\"sfr.csv.ins\"),pst_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now there are no MODFLOW-dependent pieces of the interface, so that means the forward run should be much faster...lets just run the prior parameter ensemble (where \"run\" means fill templates and process instruction files - MODFLOW is not being run!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = -1\n",
    "pst.write(os.path.join(pf.new_d,\"freyberg.pst\"),version=2)\n",
    "pr_m_d = \"master_cond_prior\"\n",
    "pyemu.os_utils.start_workers(pf.new_d,\"pestpp-ies\",\"freyberg.pst\",worker_root=\".\",num_workers=10,master_dir=pr_m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets parse out the layer-row-col on the monitoring well locations - this info in encoded in the hds obs names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_kij = hdsdf.columns.map(lambda x: tuple([int(c) for c in x.split(\"-\")[1:]])).values\n",
    "obs_kij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `kij`s are the zero-based grid index locations of the groundwater level monitoring wells.  We will now assume some HK measurements have been collected at these same grid nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.obgnme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs.loc[:,\"weight\"] = 0.0\n",
    "npfobs = obs.loc[obs.obgnme.str.startswith(\"logfreyberg6npf\"),:].copy()\n",
    "assert npfobs.shape[0] > 0\n",
    "npfobs[\"i\"] = npfobs.i.astype(int)\n",
    "npfobs[\"j\"] = npfobs.j.astype(int)\n",
    "npfobs[\"k\"] = npfobs.oname.apply(lambda x: int(x.split(\"_\")[0].split(\"layer\")[1]) - 1)\n",
    "npfobs.loc[:,\"kij\"] = npfobs.apply(lambda x: tuple([x.k,x.i,x.j]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npfobs.kij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is customary, we will use one of the prior realizations as the \"truth\" to try match later (we will also remove this realization from the prior parameter ensemble so we arent cheating...too much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_pr = pd.read_csv(os.path.join(pr_m_d,\"freyberg.0.obs.csv\"),index_col=0)\n",
    "truth_idx = oe_pr.index[0]\n",
    "truth = oe_pr.loc[truth_idx,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pe._df\n",
    "pe.drop(int(truth_idx),inplace=True)\n",
    "pe.to_csv(os.path.join(pf.new_d,\"prior.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_npfobs = npfobs.loc[npfobs.kij.isin(obs_kij),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nz_npfobs.shape[0] == obs_kij.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set both the `obsval` and `weight` quantities in the parent `pst.observation_data` dataframe.  We will assign random weights between 3 and 10, which implies a standard deviation ranging from 0.3 log HK to 0.1 log HK.  In practice, the weights (and the implied noise) of the HK data should hopefully be more than a random number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[truth.index,\"obsval\"] = truth.values\n",
    "obs.loc[nz_npfobs.obsnme,\"weight\"] = np.random.uniform(3.0,10.0,nz_npfobs.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[pst.nnz_obs_names,[\"obsval\",\"weight\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so far so fun, but now what?  Well, we can actually use PESTPP-IES to condition the prior parameter realizations to these HK estimates.  Obviously, the HK estimates should only inform HK-related parameters, so lets build a simple localizer matrix to enforce this common sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=nz_npfobs.obsnme,columns=pst.adj_par_groups,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,:] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npf_par_groups = [g for g in df.columns if \"npf\" in g]\n",
    "layer = [int(g.split(\"_\")[0].split(\"layer\")[1]) for g in npf_par_groups]\n",
    "layer_dict = {g:ll for g,ll in zip(npf_par_groups,layer)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oname in df.index:\n",
    "    k = nz_npfobs.loc[oname,\"k\"]\n",
    "    groups = [g for g in npf_par_groups if layer_dict[g]-1 == k]\n",
    "    assert len(groups) > 0\n",
    "    df.loc[oname,groups] = 1.0\n",
    "print(df.values.shape)\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,8))\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "ax.imshow(df.values.copy())\n",
    "ax.set_xticks(np.arange(df.shape[1]))\n",
    "ax.set_xticklabels(df.columns.values,rotation=90)\n",
    "ax.set_yticks(np.arange(df.shape[0]))\n",
    "ax.set_yticklabels(df.index.values)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(pf.new_d,\"loc.csv\"))\n",
    "pst.pestpp_options[\"ies_localizer\"] = \"loc.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should always run the `noptmax=0` test (and for PESTPP-IES, the `noptmax=-2` test is also good):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"ies_par_en\"] = \"prior.csv\"\n",
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(pf.new_d,\"freyberg.pst\"),version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = -2\n",
    "pst.write(os.path.join(pf.new_d,\"freyberg.pst\"),version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, here we go - feel free to change the `num_workers` arg as appropriate for your machine...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 2\n",
    "pst.write(os.path.join(pf.new_d,\"freyberg.pst\"),version=2)\n",
    "m_d = \"freyberg6_cond_master\"\n",
    "pyemu.os_utils.start_workers(pf.new_d,\"pestpp-ies\",\"freyberg.pst\",num_workers=6,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did re `phi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phidf = pd.read_csv(os.path.join(m_d,\"freyberg.phi.actual.csv\"))\n",
    "plt.plot(phidf.iteration,phidf.loc[:,\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  \n",
    "\n",
    "Now load up the observation ensembles and the obs+noise ensemebles for viz purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_dfs = []\n",
    "for iiter in range(phidf.iteration.max()+1):\n",
    "    oe_df = pd.read_csv(os.path.join(m_d,\"freyberg.{0}.obs.csv\".format(iiter)),index_col=0)\n",
    "    oe_dfs.append(oe_df)\n",
    "noise_df = pd.read_csv(os.path.join(m_d,\"freyberg.obs+noise.csv\"),index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the noise, prior, and posterior distributions at each grid node where we have an HK measurement.  If everything worked, we should see the noise (red) and posterior (blue) more or less coincident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=10\n",
    "for oname in nz_npfobs.obsnme:\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.hist(noise_df.loc[:,oname].values,bins=bins,density=True,fc=\"r\",alpha=0.5)\n",
    "    ax.hist(oe_dfs[0].loc[:,oname].values,bins=bins,density=True,fc=\"0.5\",alpha=0.5)\n",
    "    ax.hist(oe_dfs[-1].loc[:,oname].values,bins=bins,density=True,fc=\"b\",alpha=0.5)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(oname,loc=\"left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!  The posterior and noise align nicely at just about all locations, meaning we have assimilated the information in those HK measurements into the parameter ensemble.  Let's viz a realization to see a property field view of whats happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npfobs[\"i\"] = npfobs.i.astype(int)\n",
    "npfobs[\"j\"] = npfobs.j.astype(int)\n",
    "uonames = npfobs.oname.unique()\n",
    "uonames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reals = oe_dfs[-1].index.values[:2]\n",
    "for uoname in uonames:\n",
    "    uobs = npfobs.loc[npfobs.oname==uoname,:]\n",
    "    prarr = np.zeros((nrow,ncol))\n",
    "    prarr[uobs.i,uobs.j] = oe_dfs[0].loc[reals[0],uobs.obsnme]\n",
    "    prarr[id_arr==0] = np.nan\n",
    "    ptarr = np.zeros((nrow,ncol))\n",
    "    ptarr[uobs.i,uobs.j] = oe_dfs[-1].loc[reals[0],uobs.obsnme]\n",
    "    ptarr[id_arr==0] = np.nan\n",
    "    mn = min(np.nanmin(prarr),np.nanmin(ptarr))\n",
    "    mx = max(np.nanmax(prarr),np.nanmax(ptarr))\n",
    "    \n",
    "    \n",
    "    fig,axes = plt.subplots(1,2,figsize=(10,5))\n",
    "    ax = axes[0]\n",
    "    ax.set_aspect(\"equal\")\n",
    "    cb = ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,prarr,vmin=mn,vmax=mx)\n",
    "    plt.colorbar(cb,ax=ax)\n",
    "    ax.set_title(uoname+\" prior\",loc=\"left\")\n",
    "    ax = axes[1]\n",
    "    ax.set_aspect(\"equal\")\n",
    "    cb = ax.pcolormesh(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters,ptarr,vmin=mn,vmax=mx)\n",
    "    plt.colorbar(cb,ax=ax)\n",
    "    ax.set_title(uoname+\" posterior\",loc=\"left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kewl.  The layer 1 and layer 3 values have changed slightly, while the layer 2 values are unchanged (meaning the localization is working)\n",
    "At this point, you would take the final iteration parameter ensemble and use it as the prior for some history matching (or scenario analyses and/or stack in an optimization analysis if you want skip history matching!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
